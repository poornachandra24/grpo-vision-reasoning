# configs/qwen_grpo.yaml
# Optimized for MI300X + vLLM 0.11.2 + TRL 0.13.0+

# ============================================
# Identity & Artifacts
# ============================================
run_id: "Qwenv3-optimized-mi300x-full-testmini-5"  # Change this per run
hub_model_id: "Thunderbird2410/grpo-vl-lora"
wandb_project: "mi300x-qwen-grpo"
output_dir: "outputs/qwen-vl-grpo"

# Push to HuggingFace Hub after training
push_to_hub: true  # Set to false if you don't want to push

# ============================================
# Model Configuration
# ============================================
model_name: "Qwen/Qwen3-VL-8B-Instruct"
#"Qwen/Qwen2.5-VL-7B-Instruct"
attn_implementation: "flash_attention_2"

# ============================================
# LoRA Configuration
# ============================================
lora_r: 16
lora_alpha: 32  # Usually 2x lora_r, but 16 works too
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# ============================================
# Training Hyperparameters
# ============================================
num_train_epochs: 1.0
max_steps: -1
learning_rate: 5.0e-6
lr_scheduler_type: "linear"
# lr_scheduler_kwargs: null # default
warmup_ratio: 0.0
warmup_steps: 0

# Batch Configuration
batch_size: 8  # Per device batch size
gradient_accumulation_steps: 2  # Effective batch = 8 * 2 = 16

# Sequence Lengths
max_seq_length: 2048  # Max prompt length
max_completion_length: 1024  # Max generation length (NEW - was missing!)

# Generation Settings
num_generations: 8  # Number of rollouts per prompt
temperature: 1.0  # NEW - was missing
top_p: 0.9  # NEW - was missing

# ============================================
# Dataset Configuration (NEW - was missing!)
# ============================================
dataset_name: "AI4Math/MathVista"
dataset_split: "testmini"
max_samples: -1  # Use -1 for full dataset
dataset_num_proc: 16 # Number of processes for data preparation

# ============================================
# vLLM Configuration (CRITICAL)
# ============================================
use_vllm: true

# Mode: colocate = vLLM on same GPU (memory efficient)
vllm_mode: "colocate"  # NEW - was missing!

# Memory Management
vllm_gpu_memory_utilization: 0.7  # 50% for vLLM
vllm_enable_sleep_mode: true  # NEW - frees memory during training!

# vLLM Generation Settings
vllm_max_model_len: 2048  # Must match max_seq_length

# Multi-GPU (if using multiple GPUs)
vllm_tensor_parallel_size: 1  # Set to number of GPUs

# ============================================
# Optimization
# ============================================
gradient_checkpointing: true
bf16: true  # Use bfloat16

# Optimizer
# optimizer: "adamw_torch"  # Uncomment if you want to specify
# weight_decay: 0.01
# max_grad_norm: 1.0

# ============================================
# Logging & Checkpointing
# ============================================
logging_steps: 10
save_steps: 25
save_total_limit: 2
save_strategy: "steps"
logging_strategy: "steps"
report_to: "wandb"

# ============================================
# System
# ============================================
seed: 42

# ============================================
# Reward Weights (optional - for your reward functions)
# ============================================
reward_weights:
  format: 1.0
  structure: 0.5
  strict: 0.3
  correctness: 2.0

# ============================================
# Notes for MI300X
# ============================================
# 1. batch_size=4 with gradient_accumulation_steps=4 gives effective batch=16
#    - This is good for learning stability
#    - Reduce batch_size to 2 or 1 if you get OOM
#
# 2. vllm_gpu_memory_utilization=0.5 is conservative
#    - Can increase to 0.6 if you have memory to spare
#    - Decrease to 0.4 if you get OOM
#
# 3. vllm_enable_sleep_mode=true is CRITICAL
#    - Frees vLLM memory during training steps
#    - Without this, you'll likely OOM
#
# 4. max_completion_length=512 limits generation
#    - Prevents runaway generations
#    - Increase if you need longer reasoning chains
#
# 5. If you get OOM errors:
#    - Reduce batch_size to 2 or 1
#    - Reduce vllm_gpu_memory_utilization to 0.4
#    - Reduce num_generations to 2
#    - Reduce max_seq_length to 1024